{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e4de65",
   "metadata": {},
   "source": [
    "---\n",
    "pagetitle: \"W2 D1\"\n",
    "title: \"Data Work (ETL + EDA)\"\n",
    "subtitle: \"AI Professionals Bootcamp | Week 2\"\n",
    "date: 2025-12-21\n",
    "---\n",
    "\n",
    "## Policy: GenAI usage\n",
    "\n",
    "- ✅ Allowed: **clarifying questions** (definitions, error explanations)\n",
    "- ❌ Not allowed: generating code, writing solutions, or debugging by copy-paste\n",
    "- If unsure: ask the instructor first\n",
    "\n",
    "::: callout-tip\n",
    "**In this course:** you build skill by typing, running, breaking, and fixing.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Week 2 admin (what to optimize for)\n",
    "\n",
    "This week is **Data Work (ETL + EDA)**.\n",
    "\n",
    "* Internet is **not reliable** → we work **offline-first**\n",
    "* Use GitHub **daily** (small commits)\n",
    "* Prefer **one plotting library** (Plotly) later this week\n",
    "* Notebooks should read from **`data/processed/`**, not `data/raw/`\n",
    "\n",
    "::: callout-tip\n",
    "If your results change unexpectedly: check inputs, check dtypes, check joins, check row counts.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Tool stack this week (minimal + high ROI)\n",
    "\n",
    "* **pandas** — load/clean/join/reshape/EDA\n",
    "* **pyarrow + Parquet** — fast, typed processed files\n",
    "* **httpx** — extraction (but cached)\n",
    "* **logging** — visibility + audit trail\n",
    "* **Plotly** *(Day 4)* — one plotting library\n",
    "* **DuckDB** *(Day 5)* — quick local SQL on files\n",
    "\n",
    "::: {.muted}\n",
    "Rule: avoid tool sprawl. Keep it small and shippable.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Canonical workflow (repeat every project)\n",
    "\n",
    "1. **Load** (raw/cache)\n",
    "2. **Verify** (schema, dtypes, keys, row counts)\n",
    "3. **Clean** (missingness, duplicates, normalization)\n",
    "4. **Transform** (joins, reshape, features)\n",
    "5. **Analyze** (tables + comparisons)\n",
    "6. **Visualize** (Plotly, export figures)\n",
    "7. **Conclude** (written summary + caveats)\n",
    "\n",
    "::: aside\n",
    "This week: notebooks should focus on **steps 4–7** using `data/processed/`.\n",
    ":::\n",
    "\n",
    "# Day 1: Foundations for an Offline‑First Data Workflow\n",
    "\n",
    "**Goal:** set up a clean project scaffold and produce your **first processed Parquet** output (typed, reproducible).\n",
    "\n",
    "::: {.muted}\n",
    "Bootcamp • SDAIA Academy\n",
    ":::\n",
    "\n",
    "::: {.notes}\n",
    "Say: “Today is foundations. If you get the structure right, everything else becomes easier.”\n",
    "Do: show a processed Parquet file on disk + open it in pandas.\n",
    "Ask: “What’s the difference between raw and processed data?”\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Today’s Flow\n",
    "\n",
    "* **Session 1 (60m):** Offline-first mindset + project layout\n",
    "* *Asr Prayer (20m)*\n",
    "* **Session 2 (60m):** Data sources + caching patterns\n",
    "* *Maghrib Prayer (20m)*\n",
    "* **Session 3 (60m):** pandas I/O + schema basics\n",
    "* *Isha Prayer (20m)*\n",
    "* **Hands-on (120m):** Scaffold repo + load raw → processed Parquet\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of today, you can:\n",
    "\n",
    "* explain **raw vs cache vs processed** and why it matters\n",
    "* scaffold a repo with a standard **data project layout**\n",
    "* read CSV with **explicit dtypes** (avoid silent inference)\n",
    "* write **Parquet** outputs to `data/processed/`\n",
    "* implement a small **schema enforcement** step (`enforce_schema`)\n",
    "* push a working Day 1 baseline to GitHub\n",
    "\n",
    "---\n",
    "\n",
    "## Warm-up (5 minutes)\n",
    "\n",
    "Create a new Week 2 folder and confirm Python works.\n",
    "\n",
    "**macOS/Linux**\n",
    "\n",
    "```bash\n",
    "mkdir -p week2-data-work && cd week2-data-work\n",
    "python -V\n",
    "python -c \"import sys; print(sys.executable)\"\n",
    "```\n",
    "\n",
    "**Windows PowerShell**\n",
    "\n",
    "```powershell\n",
    "mkdir week2-data-work\n",
    "cd week2-data-work\n",
    "python -V\n",
    "python -c \"import sys; print(sys.executable)\"\n",
    "```\n",
    "\n",
    "**Checkpoint:** you can run Python and see a version + executable path.\n",
    "\n",
    "---\n",
    "\n",
    "## This week’s project (high-level)\n",
    "\n",
    "**Project:** *Offline‑First ETL + EDA Mini Analytics Pipeline*\n",
    "\n",
    "You will ship:\n",
    "\n",
    "* ETL code that runs end‑to‑end (load → verify → clean → transform → write)\n",
    "* `data/processed/*.parquet` outputs that are **idempotent**\n",
    "* an EDA notebook that reads **only processed** data\n",
    "* a short `reports/summary.md` (findings + caveats)\n",
    "\n",
    "---\n",
    "\n",
    "## End-state (by end of today)\n",
    "\n",
    "By the end of Day 1, your repo should contain:\n",
    "\n",
    "* a standard folder layout (`data/`, `src/`, `scripts/`, `reports/`)\n",
    "* `src/<package>/config.py` and `src/<package>/io.py`\n",
    "* a run script that writes at least one file to `data/processed/`\n",
    "* at least **one commit pushed** to GitHub\n",
    "\n",
    "# Session 1\n",
    "\n",
    "::: {.muted}\n",
    "Offline-first mindset + project layout\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Session 1 objectives\n",
    "\n",
    "By the end of this session, you can:\n",
    "\n",
    "* define **offline-first** for data work\n",
    "* explain the purpose of `raw/`, `cache/`, `processed/`\n",
    "* describe “raw immutable” and “processed idempotent”\n",
    "* centralize project paths using `pathlib.Path`\n",
    "\n",
    "---\n",
    "\n",
    "## Context: why “offline-first” is worth it\n",
    "\n",
    "You will re-run your ETL many times:\n",
    "\n",
    "* debugging\n",
    "* adding a new cleaning rule\n",
    "* fixing a dtype bug\n",
    "* adding a new feature column\n",
    "\n",
    "If your pipeline depends on the internet, you lose time.\n",
    "\n",
    "---\n",
    "\n",
    "## Concept: raw vs cache vs processed\n",
    "\n",
    "Offline-first projects separate data by **role**:\n",
    "\n",
    "* **raw:** original snapshots (never edited)\n",
    "* **cache:** downloaded/API responses (safe to delete)\n",
    "* **processed:** clean, typed outputs (safe to re-create)\n",
    "\n",
    "---\n",
    "\n",
    "## Example: folder roles (mental model)\n",
    "\n",
    ":::: {.columns}\n",
    "::: {.column width=\"30%\"}\n",
    "**`data/raw/`**\n",
    "\n",
    "* immutable inputs\n",
    "* “source of truth”\n",
    "* never edited\n",
    ":::\n",
    "\n",
    "::: {.column width=\"30%\"}\n",
    "**`data/cache/`**\n",
    "\n",
    "* API responses\n",
    "* intermediate downloads\n",
    "* safe to delete\n",
    ":::\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "**`data/processed/`**\n",
    "\n",
    "* clean + typed\n",
    "* analysis-ready\n",
    "* idempotent outputs\n",
    ":::\n",
    "::::\n",
    "\n",
    "---\n",
    "\n",
    "## Conventions that prevent pain (Day 1 baseline) {.smaller}\n",
    "\n",
    "* **Raw is immutable**: never edit files under `data/raw/`\n",
    "* **Cache is disposable**: safe to delete and re-fetch\n",
    "* **Processed is idempotent**: safe to overwrite every run\n",
    "* **One source of truth for paths**: `Path` objects + `config.py`\n",
    "* **Schema-aware**: IDs as strings; enforce dtypes after load\n",
    "* **Determinism**: if you ever simulate (bootstrap), pin a seed; record config + commit when possible\n",
    "* **Separation of concerns**:\n",
    "  * `io.py` loads/saves\n",
    "  * `transforms.py` cleans/transforms\n",
    "  * notebooks analyze/visualize\n",
    "\n",
    "::: {.notes}\n",
    "Say: “When these are violated, teams waste days debugging ghosts.”\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: classify these files (4 minutes)\n",
    "\n",
    "Put each file into the correct folder:\n",
    "\n",
    "1. `orders.csv` you received from a teammate\n",
    "2. `users_api_page_1.json` downloaded from an endpoint\n",
    "3. `orders_clean.parquet` generated by your ETL\n",
    "4. `country_codes.xlsx` you manually downloaded as reference data\n",
    "\n",
    "**Checkpoint:** you can justify your choices in 1 sentence each.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: classification\n",
    "\n",
    "1. `orders.csv` → `data/raw/`\n",
    "2. `users_api_page_1.json` → `data/cache/`\n",
    "3. `orders_clean.parquet` → `data/processed/`\n",
    "4. `country_codes.xlsx` → `data/external/` *(reference drop)*\n",
    "\n",
    "---\n",
    "\n",
    "## Context: why idempotency matters\n",
    "\n",
    "If you re-run ETL 20 times…\n",
    "\n",
    "* you **must not** duplicate rows\n",
    "* your outputs should not “drift” randomly\n",
    "* debugging should be possible\n",
    "\n",
    "Idempotent outputs make reruns safe.\n",
    "\n",
    "---\n",
    "\n",
    "## Concept: idempotent processed outputs\n",
    "\n",
    "**Idempotent:** running the pipeline again produces the same outputs (same inputs/config).\n",
    "\n",
    "Good pattern:\n",
    "\n",
    "* overwrite `data/processed/orders.parquet` every run\n",
    "\n",
    "Bad pattern:\n",
    "\n",
    "* append to `data/processed/orders.csv` every run\n",
    "\n",
    "---\n",
    "\n",
    "## Example: overwrite vs append\n",
    "\n",
    "**Bad (append)**\n",
    "\n",
    "```python\n",
    "df.to_csv(\"data/processed/orders.csv\", mode=\"a\", index=False)\n",
    "```\n",
    "\n",
    "**Good (overwrite)**\n",
    "\n",
    "```python\n",
    "df.to_parquet(\"data/processed/orders.parquet\", index=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: idempotent or not? (3 minutes)\n",
    "\n",
    "Which is idempotent?\n",
    "\n",
    "A) `df.to_csv(\"processed.csv\", mode=\"a\")`\n",
    "B) `df.to_parquet(\"processed.parquet\", index=False)` *(overwrite)*\n",
    "\n",
    "**Checkpoint:** explain what happens on the 2nd run.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: idempotent or not?\n",
    "\n",
    "* **A is NOT idempotent** (duplicates accumulate every run)\n",
    "* **B is idempotent** if the file is overwritten each run\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Check\n",
    "\n",
    "**Question:** What is the most common symptom of a non‑idempotent pipeline?\n",
    "\n",
    ". . .\n",
    "\n",
    "**Answer:** row counts grow every run, even when inputs did not change.\n",
    "\n",
    "---\n",
    "\n",
    "## Context: project layout prevents “path chaos”\n",
    "\n",
    "If everyone uses different paths:\n",
    "\n",
    "* notebooks break\n",
    "* scripts break\n",
    "* teammates can’t run your repo\n",
    "\n",
    "We fix this with a consistent layout + a central paths config.\n",
    "\n",
    "---\n",
    "\n",
    "## Concept: standard folder layout (minimum viable)\n",
    "\n",
    "A simple, robust structure:\n",
    "\n",
    "* `data/` (raw/cache/processed)\n",
    "* `src/` (reusable code)\n",
    "* `scripts/` (entrypoints)\n",
    "* `notebooks/` (EDA reads processed)\n",
    "* `reports/` (figures + summary)\n",
    "\n",
    "---\n",
    "\n",
    "## Example: repo tree (Day 1 target)\n",
    "\n",
    "```text\n",
    "week2-data-work/\n",
    "  data/{raw,cache,processed,external}/\n",
    "  notebooks/\n",
    "  reports/figures/\n",
    "  scripts/\n",
    "  src/bootcamp_data/\n",
    "    __init__.py\n",
    "    config.py\n",
    "    io.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Important: how Python finds your code (`src/` layout) {.smaller}\n",
    "\n",
    "Your package lives at:\n",
    "\n",
    "* `src/bootcamp_data/`\n",
    "\n",
    "When you run a script, Python must know where `src/` is, otherwise you’ll see:\n",
    "\n",
    "`ModuleNotFoundError: No module named 'bootcamp_data'`\n",
    "\n",
    "Two practical ways to fix it:\n",
    "\n",
    "1. **(Recommended today)** add a small `sys.path` setup at the top of scripts (shown later)\n",
    "2. **Alternative** set `PYTHONPATH=src` when running Python\n",
    "\n",
    "::: callout-tip\n",
    "If imports fail, don’t “randomly reinstall”. Fix the import path first.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: “Where does this go?” (4 minutes)\n",
    "\n",
    "Place each item:\n",
    "\n",
    "* `eda.ipynb`\n",
    "* `io.py`\n",
    "* `orders.parquet`\n",
    "* `run_day1_load.py`\n",
    "\n",
    "**Checkpoint:** you placed all 4 correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: “Where does this go?”\n",
    "\n",
    "* `eda.ipynb` → `notebooks/`\n",
    "* `io.py` → `src/bootcamp_data/`\n",
    "* `orders.parquet` → `data/processed/`\n",
    "* `run_day1_load.py` → `scripts/`\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Check\n",
    "\n",
    "**Question:** What folder should notebooks read from?\n",
    "\n",
    ". . .\n",
    "\n",
    "**Answer:** `data/processed/` (not `data/raw/`).\n",
    "\n",
    "---\n",
    "\n",
    "## Context: one source of truth for paths\n",
    "\n",
    "Hardcoding strings like `\"../data/raw/orders.csv\"` breaks when:\n",
    "\n",
    "* you move files\n",
    "* you run from a different working directory\n",
    "* someone uses Windows paths\n",
    "\n",
    "Use `pathlib.Path` + a central `config.py`.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: `Paths` + `make_paths` (pattern) {.smaller}\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    root: Path\n",
    "    raw: Path\n",
    "    cache: Path\n",
    "    processed: Path\n",
    "    external: Path\n",
    "\n",
    "def make_paths(root: Path) -> Paths:\n",
    "    data = root / \"data\"\n",
    "    return Paths(\n",
    "        root=root,\n",
    "        raw=data / \"raw\",\n",
    "        cache=data / \"cache\",\n",
    "        processed=data / \"processed\",\n",
    "        external=data / \"external\",\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: complete `make_paths` (5 minutes) {.smaller}\n",
    "\n",
    "Create `src/bootcamp_data/config.py` with:\n",
    "\n",
    "1. a `Paths` dataclass\n",
    "2. a `make_paths(root: Path)` function\n",
    "3. `raw/cache/processed/external` paths under `root/data/`\n",
    "\n",
    "**Checkpoint:** `python -c \"import sys, pathlib; sys.path.insert(0, 'src'); from bootcamp_data.config import make_paths; print(make_paths(pathlib.Path('.').resolve()).raw)\"` prints a real path.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: `config.py` (example) {.smaller}\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    root: Path\n",
    "    raw: Path\n",
    "    cache: Path\n",
    "    processed: Path\n",
    "    external: Path\n",
    "\n",
    "def make_paths(root: Path) -> Paths:\n",
    "    data = root / \"data\"\n",
    "    return Paths(\n",
    "        root=root,\n",
    "        raw=data / \"raw\",\n",
    "        cache=data / \"cache\",\n",
    "        processed=data / \"processed\",\n",
    "        external=data / \"external\",\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Check\n",
    "\n",
    "**Question:** Why use `Path` objects instead of plain strings?\n",
    "\n",
    ". . .\n",
    "\n",
    "**Answer:** `Path` handles OS differences and path joining safely (`root / \"data\" / \"raw\"`).\n",
    "\n",
    "---\n",
    "\n",
    "## Session 1 recap\n",
    "\n",
    "* Offline-first = your project runs without internet\n",
    "* Separate data by role: **raw / cache / processed**\n",
    "* Processed outputs should be **idempotent**\n",
    "* Centralize paths with `pathlib.Path` in `config.py`\n",
    "\n",
    "# Asr break {background-image='{{< brand logo anim >}}' background-opacity='0.1'}\n",
    "\n",
    "## 20 minutes\n",
    "\n",
    "**When you return:** open your repo and locate `data/raw`, `data/processed`, and `src/`.\n",
    "\n",
    "# Session 2\n",
    "\n",
    "::: {.muted}\n",
    "Data sources + caching patterns\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Session 2 objectives\n",
    "\n",
    "By the end of this session, you can:\n",
    "\n",
    "* describe common data sources (CSV/JSON/API)\n",
    "* define a minimal **extraction checklist**\n",
    "* store minimal **extraction metadata**\n",
    "* implement an offline-first **cache read/write** pattern\n",
    "\n",
    "---\n",
    "\n",
    "## Context: extraction is where “silent drift” starts\n",
    "\n",
    "Two common failure modes:\n",
    "\n",
    "* upstream data changes → your results change\n",
    "* your extraction fails partially → you analyze incomplete data\n",
    "\n",
    "Your job: make extraction reproducible.\n",
    "\n",
    "---\n",
    "\n",
    "## Concept: minimal extraction checklist\n",
    "\n",
    "Before you trust extracted data:\n",
    "\n",
    "* did you get **all pages** (pagination)?\n",
    "* did you record **params/time window**?\n",
    "* did you store a **snapshot** (cache)?\n",
    "* did you validate **row count / file size**?\n",
    "\n",
    "---\n",
    "\n",
    "## Example: minimal extraction metadata (JSON)\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"timestamp_utc\": \"2025-12-21T09:15:00Z\",\n",
    "  \"source\": \"api\",\n",
    "  \"endpoint\": \"/v1/users\",\n",
    "  \"params\": {\"page\": 1, \"per_page\": 100},\n",
    "  \"status_code\": 200\n",
    "}\n",
    "```\n",
    "\n",
    "::: {.muted}\n",
    "Store this next to the cached file (same name + `.meta.json`).\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: write metadata (5 minutes)\n",
    "\n",
    "You downloaded `data/cache/users_page_1.json`.\n",
    "\n",
    "Create a matching metadata file:\n",
    "\n",
    "* `data/cache/users_page_1.meta.json`\n",
    "* include: timestamp, endpoint, params, status_code\n",
    "\n",
    "**Checkpoint:** you have a valid JSON file on disk.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: metadata example\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"timestamp_utc\": \"2025-12-21T09:15:00Z\",\n",
    "  \"source\": \"api\",\n",
    "  \"endpoint\": \"/v1/users\",\n",
    "  \"params\": {\"page\": 1, \"per_page\": 100},\n",
    "  \"status_code\": 200\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Check\n",
    "\n",
    "**Question:** If you don’t store `params`, what breaks later?\n",
    "\n",
    ". . .\n",
    "\n",
    "**Answer:** you can’t reproduce *which* slice of data you extracted (numbers won’t match).\n",
    "\n",
    "---\n",
    "\n",
    "## Context: caching keeps you productive\n",
    "\n",
    "Internet is unreliable.\n",
    "\n",
    "Caching means:\n",
    "\n",
    "* first run: download → save to `data/cache/`\n",
    "* next runs: read from cache (fast, offline)\n",
    "\n",
    "---\n",
    "\n",
    "## Concept: offline-first caching policy\n",
    "\n",
    "Operational rule:\n",
    "\n",
    "* if cache exists → use it\n",
    "* only re-download when you *choose* Time-To-Live (TTL) or manual delete\n",
    "\n",
    "This prevents “live calls” from breaking your workflow.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: `fetch_json_cached` (offline-first) {.smaller}\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import httpx\n",
    "\n",
    "def fetch_json_cached(url: str, cache_path: Path, *, ttl_s: int | None = None) -> dict:\n",
    "    \"\"\"Offline-first JSON fetch with optional TTL.\"\"\"\n",
    "    cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Offline-first default: if cache exists, use it (unless TTL says it's too old)\n",
    "    if cache_path.exists():\n",
    "        age_s = time.time() - cache_path.stat().st_mtime\n",
    "        if ttl_s is None or age_s < ttl_s:\n",
    "            return json.loads(cache_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    # Otherwise: fetch and overwrite cache\n",
    "    with httpx.Client(timeout=20.0) as client:\n",
    "        r = client.get(url)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "\n",
    "    cache_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: predict behavior (4 minutes)\n",
    "\n",
    "Assume `cache_path` exists.\n",
    "\n",
    "What happens?\n",
    "\n",
    "1. `ttl_s=None`\n",
    "2. `ttl_s=3600` and cache age is 10 minutes\n",
    "3. `ttl_s=3600` and cache age is 3 days\n",
    "\n",
    "**Checkpoint:** you can answer all 3 without running code.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: predict behavior\n",
    "\n",
    "1. `ttl_s=None` → **read from cache** (offline-first default)\n",
    "2. `ttl_s=3600` and age 10m → **read from cache**\n",
    "3. `ttl_s=3600` and age 3d → **re-download** then overwrite cache\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Check\n",
    "\n",
    "**Question:** When is `ttl_s=None` a good default?\n",
    "\n",
    ". . .\n",
    "\n",
    "**Answer:** when you want your workflow to run offline and only refresh cache manually.\n",
    "\n",
    "---\n",
    "\n",
    "## Context: CSV is a “lowest common denominator”\n",
    "\n",
    "CSV is common, but it is fragile:\n",
    "\n",
    "* encoding surprises\n",
    "* separators differ (`;` vs `,`)\n",
    "* decimal separators differ (`1,23` vs `1.23`)\n",
    "* missing value markers vary (`NA`, `null`, empty)\n",
    "\n",
    "---\n",
    "\n",
    "## Concept: read_csv guardrails (high ROI options)\n",
    "\n",
    "When reading CSV, consider:\n",
    "\n",
    "* `dtype=` (especially IDs)\n",
    "* `na_values=` (custom missing markers)\n",
    "* `encoding=`\n",
    "* `sep=`\n",
    "* `decimal=`\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: choose options (4 minutes)\n",
    "\n",
    "Scenario:\n",
    "\n",
    "* separator is `;`\n",
    "* decimals use comma: `12,50`\n",
    "* IDs like `0007` must keep leading zeros\n",
    "\n",
    "Which options do you set?\n",
    "\n",
    "**Checkpoint:** you can write the `pd.read_csv(...)` call with those options.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: `pd.read_csv` options (example)\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\n",
    "    path,\n",
    "    sep=\";\",\n",
    "    decimal=\",\",\n",
    "    dtype={\"user_id\": \"string\", \"order_id\": \"string\"},\n",
    "    na_values=[\"\", \"NA\", \"null\"],\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Session 2 recap\n",
    "\n",
    "* Extraction must be reproducible (cache + metadata)\n",
    "* Offline-first caching: reuse cache when present\n",
    "* CSV needs guardrails (`dtype`, `sep`, `decimal`, `na_values`)\n",
    "* Next: implement typed I/O and write Parquet outputs\n",
    "\n",
    "# Maghrib break {background-image='{{< brand logo anim >}}' background-opacity='0.1'}\n",
    "\n",
    "## 20 minutes\n",
    "\n",
    "**When you return:** open VS Code (or your editor) and get ready to write `io.py`.\n",
    "\n",
    "# Session 3\n",
    "\n",
    "::: {.muted}\n",
    "pandas I/O + schema basics\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Session 3 objectives\n",
    "\n",
    "By the end of this session, you can:\n",
    "\n",
    "* explain why pandas dtype inference can be dangerous\n",
    "* read CSV with explicit dtypes (IDs as strings)\n",
    "* write and read **Parquet** with pandas\n",
    "* implement a minimal `enforce_schema(df)` transform\n",
    "\n",
    "---\n",
    "\n",
    "## Context: pandas inference can silently corrupt meaning\n",
    "\n",
    "If pandas guesses wrong, you might lose information **without an error**.\n",
    "\n",
    "Classic example:\n",
    "\n",
    "* ID `00123` becomes `123` (leading zeros lost forever)\n",
    "\n",
    "---\n",
    "\n",
    "## Concept: treat IDs as strings\n",
    "\n",
    "Operational rules:\n",
    "\n",
    "* **IDs are strings** unless you truly compute on them\n",
    "* use pandas nullable dtypes when missing values exist:\n",
    "* `\"string\"`, `\"Int64\"`, `\"Float64\"`, `\"boolean\"`\n",
    "\n",
    "---\n",
    "\n",
    "## Tiny “manual ↔ pandas” bridge {.smaller}\n",
    "\n",
    "APIs and scraped data often start as Python lists/dicts. pandas is just a convenient **table layer** on top.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# manual (list of dicts) → DataFrame\n",
    "rows = [{\"a\": 1, \"b\": \"x\"}, {\"a\": 2, \"b\": \"y\"}]\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# DataFrame → manual (records)\n",
    "records = df.to_dict(orient=\"records\")\n",
    "```\n",
    "\n",
    "Use cases:\n",
    "\n",
    "* quick debugging (`records[:3]`)\n",
    "* small unit tests\n",
    "* converting cached JSON → DataFrame\n",
    "\n",
    "---\n",
    "\n",
    "## Example: the leading-zero bug\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"orders.csv\")\n",
    "df.dtypes\n",
    "```\n",
    "\n",
    "**Risk**\n",
    "\n",
    "* `user_id` becomes `int64`\n",
    "* `0007` becomes `7`\n",
    "* joins fail later\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: choose dtypes (5 minutes)\n",
    "\n",
    "You have `orders.csv` with columns:\n",
    "\n",
    "* `order_id`, `user_id`, `amount`, `quantity`, `created_at`, `status`\n",
    "\n",
    "Write a `dtype={...}` mapping for the **IDs**.\n",
    "\n",
    "**Checkpoint:** your mapping keeps leading zeros.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: dtype mapping (IDs)\n",
    "\n",
    "```python\n",
    "dtype = {\n",
    "    \"order_id\": \"string\",\n",
    "    \"user_id\": \"string\",\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Check\n",
    "\n",
    "**Question:** If `quantity` is an integer but has missing values, what dtype should you use?\n",
    "\n",
    ". . .\n",
    "\n",
    "**Answer:** `\"Int64\"` (nullable integer), not plain `int64`.\n",
    "\n",
    "---\n",
    "\n",
    "## Context: centralize I/O in `io.py`\n",
    "\n",
    "If every notebook reads data differently:\n",
    "\n",
    "* missing values differ\n",
    "* dtypes differ\n",
    "* results differ\n",
    "\n",
    "Centralized I/O makes team work consistent.\n",
    "\n",
    "---\n",
    "\n",
    "## Concept: three core I/O helpers\n",
    "\n",
    "In `src/bootcamp_data/io.py`:\n",
    "\n",
    "* `read_orders_csv(path) -> DataFrame`\n",
    "* `read_users_csv(path) -> DataFrame`\n",
    "* `write_parquet(df, path) -> None`\n",
    "* `read_parquet(path) -> DataFrame`\n",
    "\n",
    "---\n",
    "\n",
    "## Example: `io.py` pattern\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "NA = [\"\", \"NA\", \"N/A\", \"null\", \"None\"]\n",
    "\n",
    "def read_orders_csv(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        dtype={\"order_id\": \"string\", \"user_id\": \"string\"},\n",
    "        na_values=NA,\n",
    "        keep_default_na=True,\n",
    "    )\n",
    "\n",
    "def write_parquet(df: pd.DataFrame, path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(path, index=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: pick your processed filename (3 minutes)\n",
    "\n",
    "You read `data/raw/orders.csv`.\n",
    "\n",
    "Where do you write the processed output?\n",
    "\n",
    "* folder?\n",
    "* filename?\n",
    "* format?\n",
    "\n",
    "**Checkpoint:** your answer follows the raw/cache/processed rules.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: processed output target\n",
    "\n",
    "* folder: `data/processed/`\n",
    "* filename: `orders.parquet` (or `orders_clean.parquet`)\n",
    "* format: **Parquet**\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "out_path = ROOT / \"data\" / \"processed\" / \"orders.parquet\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Check\n",
    "\n",
    "**Question:** Why Parquet instead of CSV for processed outputs?\n",
    "\n",
    ". . .\n",
    "\n",
    "**Answer:** Parquet preserves dtypes and is faster + smaller for stable processed data.\n",
    "\n",
    "---\n",
    "\n",
    "## Context: schema enforcement is a correctness step\n",
    "\n",
    "Even with `dtype=...`, you often need:\n",
    "\n",
    "* numeric parsing (`amount`, `quantity`)\n",
    "* date parsing (later)\n",
    "* consistent missing values\n",
    "\n",
    "We enforce types after loading.\n",
    "\n",
    "---\n",
    "\n",
    "## Concept: `enforce_schema(df) -> df`\n",
    "\n",
    "A simple, testable transform:\n",
    "\n",
    "* takes a DataFrame\n",
    "* casts key columns to the right dtypes\n",
    "* does not do I/O\n",
    "\n",
    "---\n",
    "\n",
    "## Example: `enforce_schema` (minimal) {.smaller}\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "def enforce_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.assign(\n",
    "        order_id=df[\"order_id\"].astype(\"string\"),\n",
    "        user_id=df[\"user_id\"].astype(\"string\"),\n",
    "        amount=pd.to_numeric(df[\"amount\"], errors=\"coerce\").astype(\"Float64\"),\n",
    "        quantity=pd.to_numeric(df[\"quantity\"], errors=\"coerce\").astype(\"Int64\"),\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: fill the blanks (4 minutes)\n",
    "\n",
    "Complete this safely:\n",
    "\n",
    "```python\n",
    "amount = pd.to_numeric(df[\"amount\"], errors=____).astype(\"Float64\")\n",
    "quantity = pd.to_numeric(df[\"quantity\"], errors=____).astype(\"Int64\")\n",
    "```\n",
    "\n",
    "**Checkpoint:** invalid values become missing (not crashes).\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: fill the blanks\n",
    "\n",
    "```python\n",
    "amount = pd.to_numeric(df[\"amount\"], errors=\"coerce\").astype(\"Float64\")\n",
    "quantity = pd.to_numeric(df[\"quantity\"], errors=\"coerce\").astype(\"Int64\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Check\n",
    "\n",
    "**Question:** What does `errors=\"coerce\"` do?\n",
    "\n",
    ". . .\n",
    "\n",
    "**Answer:** invalid values become `NaN`/missing instead of raising an exception.\n",
    "\n",
    "---\n",
    "\n",
    "## Session 3 recap\n",
    "\n",
    "* pandas dtype inference can silently break IDs\n",
    "* centralize loading/writing in `io.py`\n",
    "* write processed outputs as Parquet\n",
    "* add a small `enforce_schema` step for correctness\n",
    "\n",
    "---\n",
    "\n",
    "## Tomorrow: “Verify” becomes code (fail fast)\n",
    "\n",
    "Day 2 we’ll turn assumptions into checks, like:\n",
    "\n",
    "* required columns\n",
    "* non-empty datasets\n",
    "* unique keys (before joins)\n",
    "* missingness report (per column)\n",
    "* simple range checks (e.g., `amount >= 0`)\n",
    "\n",
    "Why it matters: catching bad data early prevents join disasters and wasted debugging.\n",
    "\n",
    "# Isha break {background-image='{{< brand logo anim >}}' background-opacity='0.1'}\n",
    "\n",
    "## 20 minutes\n",
    "\n",
    "**When you return:** we will build the repo scaffold and write our first processed Parquet file.\n",
    "\n",
    "# Hands-on\n",
    "\n",
    "::: {.muted}\n",
    "Build: scaffold repo + typed I/O + first processed output\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Hands-on success criteria (today)\n",
    "\n",
    "By the end, you should have:\n",
    "\n",
    "* a repo with the standard folder layout\n",
    "* `config.py` and `io.py` inside `src/bootcamp_data/`\n",
    "* sample raw data in `data/raw/`\n",
    "* a runnable script that writes `data/processed/orders.parquet`\n",
    "* at least **one commit pushed** to GitHub\n",
    "\n",
    "---\n",
    "\n",
    "## Project layout (target)\n",
    "\n",
    "```text\n",
    "week2-data-work/\n",
    "  data/\n",
    "    raw/            # immutable inputs\n",
    "    cache/          # API responses (optional today)\n",
    "    processed/      # your Parquet outputs\n",
    "    external/       # reference drops (optional)\n",
    "  reports/figures/\n",
    "  scripts/\n",
    "  src/bootcamp_data/\n",
    "    __init__.py\n",
    "    config.py\n",
    "    io.py\n",
    "    transforms.py   # we’ll start this today (enforce_schema)\n",
    "  README.md\n",
    "  requirements.txt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Vibe coding (safe version)\n",
    "\n",
    "1. Write the plan in 5 bullets (no code yet)\n",
    "2. Implement the smallest piece\n",
    "3. Run → break → read error → fix\n",
    "4. Commit\n",
    "5. Repeat\n",
    "\n",
    "::: callout-warning\n",
    "Do not ask GenAI to write your solution code. Ask it to explain concepts or errors.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Task 1 — Create folders + initialize git (10 minutes)\n",
    "\n",
    "* Create the repo folders (`data/`, `src/`, `scripts/`, `reports/`)\n",
    "* Initialize git\n",
    "* Create an empty `README.md`\n",
    "\n",
    "**Checkpoint:** `git status` shows your new files.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — folders + git\n",
    "\n",
    "**macOS/Linux**\n",
    "\n",
    "```bash\n",
    "mkdir -p data/{raw,cache,processed,external}\n",
    "mkdir -p reports/figures scripts src/bootcamp_data\n",
    "touch README.md src/bootcamp_data/__init__.py\n",
    "git init\n",
    "```\n",
    "\n",
    "**Windows PowerShell**\n",
    "\n",
    "```powershell\n",
    "mkdir data, reports, scripts, src\n",
    "mkdir data\\raw, data\\cache, data\\processed, data\\external\n",
    "mkdir reports\\figures\n",
    "mkdir src\\bootcamp_data\n",
    "ni README.md -ItemType File\n",
    "ni src\\bootcamp_data\\__init__.py -ItemType File\n",
    "git init\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Task 2 — Create a virtual environment + install deps (15 minutes)\n",
    "\n",
    "* Create and activate a venv\n",
    "* Install: `pandas`, `pyarrow`, `httpx`\n",
    "* Freeze `requirements.txt`\n",
    "\n",
    "**Checkpoint:** `python -c \"import pandas; import pyarrow\"` runs with no error.\n",
    "\n",
    "---\n",
    "\n",
    "## Hint — common install issues\n",
    "\n",
    "::: callout-warning\n",
    "If `pip install pyarrow` fails, you can still proceed today using CSV outputs,\n",
    "but Parquet is strongly preferred (ask the instructor for help).\n",
    ":::\n",
    "\n",
    "Also check you are inside your venv:\n",
    "\n",
    "* `which python` (macOS/Linux)\n",
    "* `Get-Command python` (Windows)\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — venv + deps\n",
    "\n",
    "**macOS/Linux**\n",
    "\n",
    "```bash\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate\n",
    "python -m pip install -U pip\n",
    "pip install pandas pyarrow httpx\n",
    "pip freeze > requirements.txt\n",
    "```\n",
    "\n",
    "**Windows PowerShell**\n",
    "\n",
    "```powershell\n",
    "python -m venv .venv\n",
    ".\\.venv\\Scripts\\Activate.ps1\n",
    "python -m pip install -U pip\n",
    "pip install pandas pyarrow httpx\n",
    "pip freeze > requirements.txt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Task 3 — Add sample raw data (10 minutes)\n",
    "\n",
    "Create two files:\n",
    "\n",
    "* `data/raw/orders.csv`\n",
    "* `data/raw/users.csv`\n",
    "\n",
    "**Checkpoint:** you can open both files and see rows + headers.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — `data/raw/orders.csv`\n",
    "\n",
    "```csv\n",
    "order_id,user_id,amount,quantity,created_at,status\n",
    "A0001,0001,12.50,1,2025-12-01T10:05:00Z,Paid\n",
    "A0002,0002,8.00,2,2025-12-01T11:10:00Z,paid\n",
    "A0003,0003,not_a_number,1,2025-12-02T09:00:00Z,Refund\n",
    "A0004,0001,25.00,,2025-12-03T14:30:00Z,PAID\n",
    "A0005,0004,100.00,1,not_a_date,paid\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — `data/raw/users.csv`\n",
    "\n",
    "```csv\n",
    "user_id,country,signup_date\n",
    "0001,SA,2025-11-15\n",
    "0002,SA,2025-11-20\n",
    "0003,AE,2025-11-22\n",
    "0004,SA,2025-11-25\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Task 4 — Implement `config.py` (15 minutes)\n",
    "\n",
    "Create `src/bootcamp_data/config.py`:\n",
    "\n",
    "* `Paths` dataclass\n",
    "* `make_paths(root: Path) -> Paths`\n",
    "\n",
    "**Checkpoint:** running a tiny import prints a valid path.\n",
    "\n",
    "---\n",
    "\n",
    "## Hint — getting the project root right\n",
    "\n",
    "In scripts, use:\n",
    "\n",
    "```python\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "```\n",
    "\n",
    "Why?\n",
    "\n",
    "* `scripts/` is one level below the repo root\n",
    "* this works even when you run scripts from different folders\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — `src/bootcamp_data/config.py` {.smaller}\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    root: Path\n",
    "    raw: Path\n",
    "    cache: Path\n",
    "    processed: Path\n",
    "    external: Path\n",
    "\n",
    "def make_paths(root: Path) -> Paths:\n",
    "    data = root / \"data\"\n",
    "    return Paths(\n",
    "        root=root,\n",
    "        raw=data / \"raw\",\n",
    "        cache=data / \"cache\",\n",
    "        processed=data / \"processed\",\n",
    "        external=data / \"external\",\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Task 5 — Implement `io.py` (20 minutes)\n",
    "\n",
    "Create `src/bootcamp_data/io.py` with:\n",
    "\n",
    "* `read_orders_csv(path: Path) -> pd.DataFrame`\n",
    "* `read_users_csv(path: Path) -> pd.DataFrame`\n",
    "* `write_parquet(df, path)`\n",
    "* `read_parquet(path)`\n",
    "\n",
    "**Checkpoint:** you can import these functions without errors.\n",
    "\n",
    "---\n",
    "\n",
    "## Hint — keep IDs as strings\n",
    "\n",
    "In both CSV readers:\n",
    "\n",
    "* set `dtype={\"user_id\": \"string\"}` (and `order_id` for orders)\n",
    "* define `NA = [...]` once\n",
    "\n",
    "::: callout-tip\n",
    "Centralizing `NA` markers makes your entire project consistent.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — `src/bootcamp_data/io.py` {.smaller}\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "NA = [\"\", \"NA\", \"N/A\", \"null\", \"None\"]\n",
    "def read_orders_csv(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        dtype={\"order_id\": \"string\", \"user_id\": \"string\"},\n",
    "        na_values=NA,\n",
    "        keep_default_na=True,\n",
    "    )\n",
    "def read_users_csv(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        dtype={\"user_id\": \"string\"},\n",
    "        na_values=NA,\n",
    "        keep_default_na=True,\n",
    "    )\n",
    "def write_parquet(df: pd.DataFrame, path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(path, index=False)\n",
    "\n",
    "def read_parquet(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_parquet(path)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Task 6 — Add `transforms.py` with `enforce_schema` (15 minutes)\n",
    "\n",
    "Create `src/bootcamp_data/transforms.py`:\n",
    "\n",
    "* implement `enforce_schema(df) -> df`\n",
    "* convert `amount` and `quantity` using `pd.to_numeric(..., errors=\"coerce\")`\n",
    "\n",
    "**Checkpoint:** calling `enforce_schema` returns a DataFrame with `Float64` / `Int64` dtypes.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — `src/bootcamp_data/transforms.py` (minimal) {.smaller}\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "def enforce_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.assign(\n",
    "        order_id=df[\"order_id\"].astype(\"string\"),\n",
    "        user_id=df[\"user_id\"].astype(\"string\"),\n",
    "        amount=pd.to_numeric(df[\"amount\"], errors=\"coerce\").astype(\"Float64\"),\n",
    "        quantity=pd.to_numeric(df[\"quantity\"], errors=\"coerce\").astype(\"Int64\"),\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Task 7 — Write a Day 1 run script (20 minutes)\n",
    "\n",
    "Create `scripts/run_day1_load.py`:\n",
    "\n",
    "* load paths from `make_paths(ROOT)`\n",
    "* read raw CSVs\n",
    "* apply `enforce_schema` to orders\n",
    "* write Parquet outputs to `data/processed/`\n",
    "\n",
    "**Checkpoint:** running the script creates `data/processed/orders.parquet`.\n",
    "\n",
    "---\n",
    "\n",
    "## Hint — your script should log evidence\n",
    "\n",
    "Log (or print) at least:\n",
    "\n",
    "* row counts\n",
    "* dtypes\n",
    "* output paths\n",
    "\n",
    "This helps debugging later.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — `scripts/run_day1_load.py` {.smaller auto-animate=true}\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "import logging\n",
    "\n",
    "# Make `src/` importable when running as a script\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "SRC = ROOT / \"src\"\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "from bootcamp_data.config import make_paths\n",
    "from bootcamp_data.io import read_orders_csv, read_users_csv, write_parquet\n",
    "from bootcamp_data.transforms import enforce_schema\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "def main() -> None:\n",
    "    ... # continue on the next slide\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — `scripts/run_day1_load.py` {.smaller auto-animate=true}\n",
    "\n",
    "```python\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(levelname)s %(name)s: %(message)s\")\n",
    "\n",
    "    p = make_paths(ROOT)\n",
    "    orders = enforce_schema(read_orders_csv(p.raw / \"orders.csv\"))\n",
    "    users = read_users_csv(p.raw / \"users.csv\")\n",
    "\n",
    "    log.info(\"Loaded rows: orders=%s users=%s\", len(orders), len(users))\n",
    "    log.info(\"Orders dtypes:\\n%s\", orders.dtypes)\n",
    "\n",
    "    out_orders = p.processed / \"orders.parquet\"\n",
    "    out_users = p.processed / \"users.parquet\"\n",
    "    write_parquet(orders, out_orders)\n",
    "    write_parquet(users, out_users)\n",
    "\n",
    "    meta = {  # Optional but useful: minimal run metadata for reproducibility\n",
    "        \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"rows\": {\"orders\": int(len(orders)), \"users\": int(len(users))},\n",
    "        \"outputs\": {\"orders\": str(out_orders), \"users\": str(out_users)},\n",
    "    }\n",
    "    meta_path = p.processed / \"_run_meta.json\"\n",
    "    meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    log.info(\"Wrote: %s\", p.processed)\n",
    "    log.info(\"Run meta: %s\", meta_path)\n",
    "```\n",
    "\n",
    "::: aside\n",
    "This is the body of `main()` defined in the previous slide.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Task 8 — Run + verify outputs (10 minutes)\n",
    "\n",
    "Run the script and verify:\n",
    "\n",
    "* processed files exist\n",
    "* IDs stayed as strings\n",
    "* `amount` has missing values for invalid rows\n",
    "\n",
    "**Checkpoint:** you can load the Parquet file back into pandas.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — run + verify\n",
    "\n",
    "```bash\n",
    "python scripts/run_day1_load.py\n",
    "python -c \"import pandas as pd; \\\n",
    "           df=pd.read_parquet('data/processed/orders.parquet'); \\\n",
    "           print(df.dtypes); \\\n",
    "           print(df.head())\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Git checkpoint (5 minutes)\n",
    "\n",
    "* `git status`\n",
    "* commit with message: `\"w2d1: scaffold + typed io + first processed parquet\"`\n",
    "* push to GitHub\n",
    "\n",
    "**Checkpoint:** you can see your commit online.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — git commands\n",
    "\n",
    "```bash\n",
    "git add -A\n",
    "git commit -m \"w2d1: scaffold + typed io + first processed parquet\"\n",
    "git branch -M main\n",
    "git remote add origin <YOUR_REPO_URL>\n",
    "git push -u origin main\n",
    "```\n",
    "\n",
    "::: {.muted}\n",
    "If you already have a remote, skip the `git remote add` step.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Debug playbook\n",
    "\n",
    "When stuck:\n",
    "\n",
    "1. Read the full error (don’t guess)\n",
    "2. Identify: file + line number\n",
    "3. Print: `paths`, row counts, `df.dtypes`\n",
    "4. Fix the smallest thing\n",
    "5. Re-run\n",
    "\n",
    "::: callout-tip\n",
    "Most Day 1 bugs are: wrong working directory, missing venv, missing dependency, or wrong import path.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Stretch goals (optional)\n",
    "\n",
    "If you finish early:\n",
    "\n",
    "* write `_run_meta.json` with row counts + output paths\n",
    "* add a `README.md` “How to run Day 1”\n",
    "* add a tiny check: assert `orders[\"user_id\"].dtype == \"string\"`\n",
    "\n",
    "---\n",
    "\n",
    "## Exit Ticket\n",
    "\n",
    "In 1–2 sentences:\n",
    "\n",
    "**Why do we keep IDs as strings and prefer Parquet for processed outputs?**\n",
    "\n",
    "---\n",
    "\n",
    "## What to do after class (Day 1 assignment)\n",
    "\n",
    "**Due:** before Day 2 starts\n",
    "\n",
    "1. Clean up your repo so it matches the target layout\n",
    "2. Ensure `scripts/run_day1_load.py` runs from a fresh terminal\n",
    "3. Push at least **one** commit to GitHub\n",
    "\n",
    "**Deliverable:** GitHub repo link + screenshot showing `data/processed/orders.parquet`.\n",
    "\n",
    "::: callout-tip\n",
    "Commit early. Commit often. Future you will thank you.\n",
    ":::\n",
    "\n",
    "# Thank You! {background-image='{{< brand logo anim >}}' background-opacity='0.1'}\n",
    "\n",
    "<div style=\"width: 300px\">{{< brand logo full >}}</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360562d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
